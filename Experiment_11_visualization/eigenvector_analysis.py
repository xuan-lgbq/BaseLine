import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import pickle
import json

def get_layer_parameter_info(model):
    """è·å–æ¯å±‚å‚æ•°çš„è¯¦ç»†ä¿¡æ¯"""
    layer_info = []
    start_idx = 0
    
    for i, (name, param) in enumerate(model.named_parameters()):
        if param.requires_grad:
            param_count = param.numel()
            layer_info.append({
                'layer_idx': i,
                'name': name,
                'start_idx': start_idx,
                'end_idx': start_idx + param_count,
                'param_count': param_count,
                'shape': param.shape,
                'layer_type': 'weight' if 'weight' in name else 'bias'
            })
            start_idx += param_count
    
    return layer_info

def compute_layerwise_frobenius_norms(eigenvectors, layer_info, dominant_dim):
    """è®¡ç®—å‰dominant_dimä¸ªç‰¹å¾å‘é‡åœ¨å„å±‚çš„FrobeniusèŒƒæ•°"""
    # ç¡®ä¿åªä½¿ç”¨å‰dominant_dimä¸ªç‰¹å¾å‘é‡
    if eigenvectors.dim() == 1:
        eigenvectors = eigenvectors.unsqueeze(1)
    
    num_eigenvectors = min(dominant_dim, eigenvectors.shape[1])
    eigenvectors = eigenvectors[:, :num_eigenvectors]
    
    print(f"ğŸ” è®¡ç®—å‰{num_eigenvectors}ä¸ªç‰¹å¾å‘é‡çš„åˆ†å±‚FèŒƒæ•°")
    
    # å­˜å‚¨ç»“æœ
    layer_norms = {}  # {layer_idx: [norm_eigen1, norm_eigen2, ...]}
    
    for layer in layer_info:
        layer_idx = layer['layer_idx']
        start_idx = layer['start_idx']
        end_idx = layer['end_idx']
        
        # æå–è¯¥å±‚å¯¹åº”çš„ç‰¹å¾å‘é‡éƒ¨åˆ†
        layer_eigenvectors = eigenvectors[start_idx:end_idx, :]
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾å‘é‡åœ¨è¯¥å±‚çš„FrobeniusèŒƒæ•°
        layer_norms[layer_idx] = []
        for k in range(num_eigenvectors):
            eigenve_layer_part = layer_eigenvectors[:, k]
            frobenius_norm = torch.norm(eigenve_layer_part, p='fro').item()
            layer_norms[layer_idx].append(frobenius_norm)
            
        print(f"  Layer {layer_idx} ({layer['name']}): F-norms = {layer_norms[layer_idx]}")
    
    return layer_norms

def plot_layerwise_frobenius_norms(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step):
    """å¯è§†åŒ–å‰dominant_dimä¸ªç‰¹å¾å‘é‡åœ¨å„å±‚çš„FrobeniusèŒƒæ•°"""
    num_layers = len(layer_info)
    num_eigenvectors = min(dominant_dim, len(next(iter(layer_norms.values()))))
    
    plt.figure(figsize=(14, 10))
    
    # å‡†å¤‡æ•°æ®
    layer_indices = list(range(num_layers))
    layer_names = [layer['name'] for layer in layer_info]
    
    # ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡ç»˜åˆ¶ä¸€æ¡çº¿
    colors = plt.cm.tab10(np.linspace(0, 1, min(num_eigenvectors, 10)))
    
    for k in range(num_eigenvectors):
        norms = [layer_norms[layer_idx][k] for layer_idx in range(num_layers)]
        
        eigenval = eigenvalues[k].item() if hasattr(eigenvalues[k], 'item') else eigenvalues[k]
        label = f'ç‰¹å¾å‘é‡ {k+1} (Î»={eigenval:.4f})'
        
        plt.plot(layer_indices, norms, 
                'o-', 
                color=colors[k % len(colors)], 
                linewidth=2, 
                markersize=6,
                label=label)
    
    plt.xlabel('å±‚æ•°', fontsize=14)
    plt.ylabel('Frobenius èŒƒæ•°', fontsize=14)
    plt.title(f'å‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡åœ¨å„å±‚çš„FrobeniusèŒƒæ•°åˆ†å¸ƒ (Step {step})\nDominant Dimension = {dominant_dim}', fontsize=16)
    
    # è®¾ç½®xè½´æ ‡ç­¾
    plt.xticks(layer_indices, [f'Layer {i+1}\n{name.split(".")[-1]}' for i, name in enumerate(layer_names)], 
               rotation=45, ha='right')
    
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡åˆ†å±‚åˆ†æå›¾å·²ä¿å­˜: {save_path}")
    plt.close()

def plot_layerwise_frobenius_norms(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step):
    """å¯è§†åŒ–å‰dominant_dimä¸ªç‰¹å¾å‘é‡åœ¨å„å±‚çš„FrobeniusèŒƒæ•° - æŸ±çŠ¶å›¾ç‰ˆæœ¬"""
    num_layers = len(layer_info)
    num_eigenvectors = min(dominant_dim, len(next(iter(layer_norms.values()))))
    
    # åˆ›å»ºæ›´å¤§çš„å›¾åƒä»¥å®¹çº³æ›´å¤šä¿¡æ¯
    fig, ax = plt.subplots(figsize=(16, 10))
    
    # å‡†å¤‡æ•°æ®
    layer_names = [f"Layer {i}\n{layer['name'].split('.')[-1]}" for i, layer in enumerate(layer_info)]
    x = np.arange(num_layers)  # å±‚çš„ä½ç½®
    
    # è®¡ç®—æŸ±å­å®½åº¦
    width = 0.8 / num_eigenvectors if num_eigenvectors > 0 else 0.8
    
    # é¢œè‰²æ˜ å°„
    colors = plt.cm.Set3(np.linspace(0, 1, num_eigenvectors))
    
    # ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡ç»˜åˆ¶æŸ±çŠ¶å›¾
    for k in range(num_eigenvectors):
        # è·å–è¯¥ç‰¹å¾å‘é‡åœ¨å„å±‚çš„FèŒƒæ•°
        norms = [layer_norms[layer_idx][k] for layer_idx in range(num_layers)]
        
        # è®¡ç®—æŸ±å­ä½ç½®ï¼ˆç›¸å¯¹äºä¸­å¿ƒåç§»ï¼‰
        offset = (k - (num_eigenvectors - 1) / 2) * width
        x_pos = x + offset
        
        # è·å–ç‰¹å¾å€¼
        eigenval = eigenvalues[k].item() if hasattr(eigenvalues[k], 'item') else eigenvalues[k]
        label = f'Eigenvector {k+1} (Î»={eigenval:.4f})'
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars = ax.bar(x_pos, norms, width, 
                     color=colors[k], 
                     alpha=0.8,
                     label=label,
                     edgecolor='black',
                     linewidth=0.5)
        
        # åœ¨æŸ±å­é¡¶éƒ¨æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, norm) in enumerate(zip(bars, norms)):
            if norm > 0:  # åªåœ¨æœ‰å€¼çš„æŸ±å­ä¸Šæ·»åŠ æ ‡ç­¾
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(norms)*0.01,
                       f'{norm:.3f}', 
                       ha='center', va='bottom', 
                       fontsize=8, 
                       rotation=90 if num_eigenvectors > 3 else 0)
    
    # è®¾ç½®åæ ‡è½´
    ax.set_xlabel('Network Layers', fontsize=14, fontweight='bold')  # æ”¹ä¸ºè‹±æ–‡
    ax.set_ylabel('Frobenius Norm', fontsize=14, fontweight='bold')  # æ”¹ä¸ºè‹±æ–‡
    ax.set_title(f'Frobenius Norms of Top {dominant_dim} Eigenvectors Across Layers (Step {step})\n'
                f'Dominant Dimension = {dominant_dim}', 
                fontsize=16, fontweight='bold', pad=20)  # æ”¹ä¸ºè‹±æ–‡
    
    # è®¾ç½®xè½´åˆ»åº¦å’Œæ ‡ç­¾
    ax.set_xticks(x)
    ax.set_xticklabels(layer_names, fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--', axis='y')
    ax.set_axisbelow(True)
    
    # è®¾ç½®å›¾ä¾‹
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡åˆ†å±‚æŸ±çŠ¶å›¾å·²ä¿å­˜: {save_path}")
    plt.close()

def plot_layerwise_frobenius_norms_stacked(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step):
    """å †å æŸ±çŠ¶å›¾ç‰ˆæœ¬ - æ›´é€‚åˆæ¯”è¾ƒä¸åŒç‰¹å¾å‘é‡çš„ç›¸å¯¹è´¡çŒ®"""
    num_layers = len(layer_info)
    num_eigenvectors = min(dominant_dim, len(next(iter(layer_norms.values()))))
    
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # å‡†å¤‡æ•°æ®
    layer_names = [f"Layer {i}\n{layer['name'].split('.')[-1]}" for i, layer in enumerate(layer_info)]
    
    # æ„å»ºæ•°æ®çŸ©é˜µ (num_eigenvectors x num_layers)
    data_matrix = np.zeros((num_eigenvectors, num_layers))
    labels = []
    
    for k in range(num_eigenvectors):
        for layer_idx in range(num_layers):
            data_matrix[k, layer_idx] = layer_norms[layer_idx][k]
        
        eigenval = eigenvalues[k].item() if hasattr(eigenvalues[k], 'item') else eigenvalues[k]
        labels.append(f'Eigenvector {k+1} (Î»={eigenval:.4f})')
    
    # é¢œè‰²æ˜ å°„
    colors = plt.cm.Set3(np.linspace(0, 1, num_eigenvectors))
    
    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    x = np.arange(num_layers)
    bottom = np.zeros(num_layers)
    
    for k in range(num_eigenvectors):
        ax.bar(x, data_matrix[k], 
               color=colors[k], 
               alpha=0.8,
               label=labels[k],
               bottom=bottom,
               edgecolor='black',
               linewidth=0.5)
        bottom += data_matrix[k]
    
    # è®¾ç½®åæ ‡è½´
    ax.set_xlabel('Network Layers', fontsize=14, fontweight='bold')  # æ”¹ä¸ºè‹±æ–‡
    ax.set_ylabel('Frobenius Norm', fontsize=14, fontweight='bold')  # æ”¹ä¸ºè‹±æ–‡
    ax.set_title(f'Frobenius Norms of Top {dominant_dim} Eigenvectors Across Layers (Step {step})\n'
                f'Dominant Dimension = {dominant_dim}', 
                fontsize=16, fontweight='bold', pad=20)  # æ”¹ä¸ºè‹±æ–‡
    
    # è®¾ç½®xè½´
    ax.set_xticks(x)
    ax.set_xticklabels(layer_names, fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼å’Œå›¾ä¾‹
    ax.grid(True, alpha=0.3, linestyle='--', axis='y')
    ax.set_axisbelow(True)
    
    # è®¾ç½®å›¾ä¾‹
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(save_path.replace('.png', '_stacked.png'), dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å †å æŸ±çŠ¶å›¾å·²ä¿å­˜: {save_path.replace('.png', '_stacked.png')}")
    plt.close()

def plot_layerwise_frobenius_norms_combined(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step):
    """ç»„åˆç‰ˆæœ¬ï¼šåŒæ—¶ç”Ÿæˆæ™®é€šæŸ±çŠ¶å›¾å’Œå †å æŸ±çŠ¶å›¾"""
    # ç”Ÿæˆæ™®é€šæŸ±çŠ¶å›¾
    plot_layerwise_frobenius_norms(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step)
    
    # ç”Ÿæˆå †å æŸ±çŠ¶å›¾
    plot_layerwise_frobenius_norms_stacked(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step)
    
    # ç”Ÿæˆçƒ­åŠ›å›¾ç‰ˆæœ¬
    plot_layerwise_frobenius_heatmap(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step)

def plot_layerwise_frobenius_heatmap(layer_norms, layer_info, eigenvalues, dominant_dim, save_path, step):
    """çƒ­åŠ›å›¾ç‰ˆæœ¬ - é€‚åˆæŸ¥çœ‹æ¨¡å¼"""
    num_layers = len(layer_info)
    num_eigenvectors = min(dominant_dim, len(next(iter(layer_norms.values()))))
    
    # æ„å»ºæ•°æ®çŸ©é˜µ
    data_matrix = np.zeros((num_eigenvectors, num_layers))
    
    for k in range(num_eigenvectors):
        for layer_idx in range(num_layers):
            data_matrix[k, layer_idx] = layer_norms[layer_idx][k]
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = ax.imshow(data_matrix, cmap='YlOrRd', aspect='auto')
    
    # è®¾ç½®åæ ‡è½´
    layer_names = [f"Layer {i}\n{layer['name'].split('.')[-1]}" for i, layer in enumerate(layer_info)]
    eigenvector_names = []
    for k in range(num_eigenvectors):
        eigenval = eigenvalues[k].item() if hasattr(eigenvalues[k], 'item') else eigenvalues[k]
        eigenvector_names.append(f'EigenVec {k+1}\n(Î»={eigenval:.3f})')
    
    ax.set_xticks(np.arange(num_layers))
    ax.set_yticks(np.arange(num_eigenvectors))
    ax.set_xticklabels(layer_names)
    ax.set_yticklabels(eigenvector_names)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in range(num_eigenvectors):
        for j in range(num_layers):
            text = ax.text(j, i, f'{data_matrix[i, j]:.3f}',
                         ha="center", va="center", color="black", fontsize=10)
    
    ax.set_title(f'Eigenvector Layerwise Frobenius Norm Heatmap (Step {step})\n'
                f'Dominant Dimension = {dominant_dim}', 
                fontsize=16, fontweight='bold', pad=20)  # æ”¹ä¸ºè‹±æ–‡
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Frobenius Norm', fontsize=12)  # æ”¹ä¸ºè‹±æ–‡
    
    plt.tight_layout()
    plt.savefig(save_path.replace('.png', '_heatmap.png'), dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path.replace('.png', '_heatmap.png')}")
    plt.close()
    
def save_eigenvector_data(analysis_data, save_dir, step, method, lr, var, seed, rank, num_layer):
    """ä¿å­˜ç‰¹å¾å‘é‡ç›¸å…³æ•°æ®"""
    if analysis_data is None:
        return
    
    # åˆ›å»ºä¿å­˜æ–‡ä»¶å
    base_filename = f"eigenvectors_step_{step}_{method}_lr{lr}_var{var:.6f}_seed{seed}_rank{rank}_layer{num_layer}"
    
    # 1. ä¿å­˜ä¸ºPyTorchæ ¼å¼ (.ptæ–‡ä»¶)
    torch_save_path = os.path.join(save_dir, f"{base_filename}.pt")
    torch_data = {
        'step': step,
        'eigenvalues': analysis_data['eigenvalues'],
        'eigenvectors': analysis_data['eigenvectors'],
        'dominant_dim': analysis_data['dominant_dim'],
        'layer_norms': analysis_data['layer_norms'],
        'layer_info': analysis_data['layer_info'],
        'metadata': {
            'method': method,
            'learning_rate': lr,
            'variance': var,
            'seed': seed,
            'rank': rank,
            'num_layer': num_layer,
            'timestamp': step
        }
    }
    torch.save(torch_data, torch_save_path)
    print(f"ğŸ’¾ ç‰¹å¾å‘é‡æ•°æ®å·²ä¿å­˜: {torch_save_path}")
    
    # 2. ä¿å­˜åˆ†å±‚åˆ†æç»“æœä¸ºJSONæ ¼å¼
    json_save_path = os.path.join(save_dir, f"{base_filename}_layer_analysis.json")
    json_data = {
        'step': step,
        'dominant_dim': analysis_data['dominant_dim'],
        'eigenvalues': [float(ev) for ev in analysis_data['eigenvalues']],
        'layer_norms': {str(k): [float(norm) for norm in v] for k, v in analysis_data['layer_norms'].items()},
        'layer_info': [
            {
                'layer_idx': layer['layer_idx'],
                'name': layer['name'],
                'start_idx': layer['start_idx'],
                'end_idx': layer['end_idx'],
                'param_count': layer['param_count'],
                'shape': list(layer['shape']),
                'layer_type': layer['layer_type']
            }
            for layer in analysis_data['layer_info']
        ],
        'parameter_ranges': analysis_data.get('parameter_ranges', {}),
        'metadata': {
            'method': method,
            'learning_rate': lr,
            'variance': var,
            'seed': seed,
            'rank': rank,
            'num_layer': num_layer
        }
    }
    
    with open(json_save_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ åˆ†å±‚åˆ†ææ•°æ®å·²ä¿å­˜: {json_save_path}")
    
    # 3. ä¿å­˜ç‰¹å¾å‘é‡ä¸ºnumpyæ ¼å¼ (ä¾¿äºå…¶ä»–å·¥å…·è¯»å–)
    numpy_save_path = os.path.join(save_dir, f"{base_filename}_eigenvectors.npz")
    np.savez(numpy_save_path,
             eigenvalues=analysis_data['eigenvalues'].cpu().numpy(),
             eigenvectors=analysis_data['eigenvectors'].cpu().numpy(),
             dominant_dim=analysis_data['dominant_dim'],
             step=step)
    print(f"ğŸ’¾ ç‰¹å¾å‘é‡numpyæ•°æ®å·²ä¿å­˜: {numpy_save_path}")
    
    return {
        'torch_path': torch_save_path,
        'json_path': json_save_path,
        'numpy_path': numpy_save_path
    }

def load_eigenvector_data(file_path):
    """åŠ è½½å·²ä¿å­˜çš„ç‰¹å¾å‘é‡æ•°æ®"""
    if file_path.endswith('.pt'):
        data = torch.load(file_path)
        print(f"ğŸ“‚ å·²åŠ è½½ç‰¹å¾å‘é‡æ•°æ®: {file_path}")
        return data
    elif file_path.endswith('.npz'):
        data = np.load(file_path)
        print(f"ğŸ“‚ å·²åŠ è½½numpyç‰¹å¾å‘é‡æ•°æ®: {file_path}")
        return data
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .pt æˆ– .npz æ–‡ä»¶")

def analyze_dominant_space_layerwise(model, eigenvectors, eigenvalues, dominant_dim, save_dir, step):
    """åˆ†æå‰dominant_dimä¸ªç‰¹å¾å‘é‡çš„åˆ†å±‚åˆ†å¸ƒ"""
    print(f"ğŸ¨ Step {step}: å¼€å§‹åˆ†æå‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡çš„åˆ†å±‚åˆ†å¸ƒ...")
    
    if eigenvectors is None or eigenvalues is None:
        print("âŒ ç‰¹å¾å‘é‡æˆ–ç‰¹å¾å€¼ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
        return None
    
    # è·å–å±‚ä¿¡æ¯
    layer_info = get_layer_parameter_info(model)
    
    # è®¡ç®—åˆ†å±‚FrobeniusèŒƒæ•°
    layer_norms = compute_layerwise_frobenius_norms(eigenvectors, layer_info, dominant_dim)
    
    # å¯è§†åŒ– - ç”Ÿæˆå¤šç§å›¾è¡¨
    base_save_path = os.path.join(save_dir, f"eigenvector_layerwise_step_{step}_dom{dominant_dim}")
    
    # 1. æ™®é€šæŸ±çŠ¶å›¾
    bar_save_path = f"{base_save_path}_bar.png"
    plot_layerwise_frobenius_norms(layer_norms, layer_info, eigenvalues, dominant_dim, bar_save_path, step)
    
    # 2. å †å æŸ±çŠ¶å›¾
    stacked_save_path = f"{base_save_path}_stacked.png"
    plot_layerwise_frobenius_norms_stacked(layer_norms, layer_info, eigenvalues, dominant_dim, stacked_save_path, step)
    
    # 3. çƒ­åŠ›å›¾
    heatmap_save_path = f"{base_save_path}_heatmap.png"
    plot_layerwise_frobenius_heatmap(layer_norms, layer_info, eigenvalues, dominant_dim, heatmap_save_path, step)
    
    # ä¿å­˜æ•°æ®
    analysis_data = {
        'eigenvalues': eigenvalues[:dominant_dim],
        'eigenvectors': eigenvectors[:, :dominant_dim] if eigenvectors.dim() > 1 else eigenvectors,
        'layer_norms': layer_norms,
        'layer_info': layer_info,
        'dominant_dim': dominant_dim,
        'parameter_ranges': {f"Layer_{i}": f"[{info['start_idx']}:{info['end_idx']})" 
                            for i, info in enumerate(layer_info)}
    }
    
    print(f"âœ… Step {step}: å‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡åˆ†å±‚åˆ†æå®Œæˆ")
    print(f"ğŸ“Š ç”Ÿæˆäº†3ç§å›¾è¡¨: æŸ±çŠ¶å›¾ã€å †å æŸ±çŠ¶å›¾ã€çƒ­åŠ›å›¾")
    
    return analysis_data