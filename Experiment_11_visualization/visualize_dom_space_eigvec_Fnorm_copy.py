import torch
import torch.nn as nn
import numpy as np
import swanlab as wandb
import time
from datetime import datetime
from tqdm import tqdm
import os
from torch.utils.data import TensorDataset, DataLoader

# Configuration imports
from config_linear import training_config as config
from config_linear import device

# Model imports
from model_linear import LinearNetwork
from generate_low_rank import generate_low_rank_matrix, generative_dataset

# Custom modules
from training_utils import (
    set_seed, format_time, generate_eigenvalue_steps, 
    prepare_data_dimensions, print_training_info
)
from eigenvalue_analysis import (
    compute_and_analyze_eigenvalues, collect_eigenvalue_data, 
    prepare_wandb_log
)
from plotting import (
    plot_training_loss, plot_top_k_eigenvalues, 
    plot_dominant_dims, save_eigenvalue_csv
)

# æ–°å¢æ¨¡å—
from hessian_visualization import visualize_hessian_structure
from eigenvector_analysis import analyze_dominant_space_layerwise
from parameter_utils import get_parameter_ordering
from eigenvector_analysis import analyze_dominant_space_layerwise, save_eigenvector_data

from create_directory import create_experiment_directories  # åªæ·»åŠ è¿™ä¸ªå¯¼å…¥

# åˆ›å»ºå›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹
# æ·»åŠ è¿™ä¸€è¡Œ - åˆ›å»ºå®éªŒç›®å½•
directories, experiment_name = create_experiment_directories(config)
IMAGE_SAVE_DIR = directories['images']  # æ›¿æ¢åŸæ¥çš„è·¯å¾„
os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)
print(f"ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°: {IMAGE_SAVE_DIR}")

def main():
    # åˆå§‹åŒ–æ¨¡å‹
    model = LinearNetwork(config["input_dim"], config["hidden_dim"], config["output_dim"], config["num_layer"], config["variance"], device).to(device)
    
    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params}")
    
    # Setup SGD optimizer (simplified)
    optimizer = torch.optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)
    method = "SGD"
    print("Using SGD optimizer")
    
    # Get configuration parameters
    lr = config["learning_rate"]
    var = model.var
    top_k = config["top_k_pca_number"]
    rank = config.get("rank", 5)
    num_layer = config.get("num_layer", 3)
    output_dim = config.get("output_dim", 10)
    eigenvalue_interval = config.get("eigenvalue_interval", 25)
    steps = config["steps"]
    
    print(f"Learning rate: {lr}")
    print(f"Variance: {var}")
    print(f"Computing top {top_k} Hessian eigenvalues")
    print(f"Rank: {rank}")
    print(f"ç‰¹å¾å€¼è®¡ç®—é—´éš”: æ¯ {eigenvalue_interval} æ­¥")
    
    # Generate eigenvalue computation steps
    selected_steps = generate_eigenvalue_steps(steps, eigenvalue_interval)
    print(f"ç‰¹å¾å€¼è®¡ç®—æ­¥éª¤: {len(selected_steps)} ä¸ªè®¡ç®—ç‚¹")
    
    # Initialize wandb
    gap_method = config["method"]
    wandb.init(project=config["swanlab_project_name"], 
               name=f"Linear{num_layer}+{gap_method}+{method}+outputdim{output_dim}+lr{lr}+var{var:.6f}_rank{rank}_top{top_k}", 
               api_key="zrVzavwSxtY7Gs0GWo9xV")
    
    wandb.config.update(config)
    wandb.config.update({
        "optimizer": method,
        "eigenvalue_interval": eigenvalue_interval,
        "target_rank": rank
    })
    
    loss_function = nn.MSELoss(reduction='mean')
    
    # Training setup
    seed = 12138
    total_start_time = time.time()
    print_training_info(steps, [seed], selected_steps)
    
    # Set seed
    set_seed(seed)
    print(f"\nğŸŒ± Using seed: {seed}")
    
    # Initialize data collection variables
    eigenvalue_history = {f"top_{i+1}": [] for i in range(top_k)}
    step_history = []
    dominant_dim_history = []
    eigenvalue_step_history = []
    loss_history = []
    
    # Generate and prepare data
    data, label = generative_dataset(config["input_dim"], config["output_dim"], use_custom_rank=True)
    data, label = prepare_data_dimensions(data, label, device)
    
    # Print low rank matrix info
    projection_matrix = generate_low_rank_matrix(config["input_dim"], config["output_dim"])
    print(f"\nğŸ“Š Low Rank Matrix (rank={config['rank']}):")
    print(f"çŸ©é˜µå½¢çŠ¶: {projection_matrix.shape}")
    print(f"å®é™…ç§©: {torch.linalg.matrix_rank(projection_matrix)}")
    
    single_loader = DataLoader(TensorDataset(data, label), batch_size=1, shuffle=False)
    
    # Training loop
    progress_bar = tqdm(range(steps + 1), 
                       desc="Training", 
                       ncols=100)

    for step in progress_bar:
        step_start_time = time.time()
        
        # Forward pass
        output = model.forward(data)
        
        # Ensure dimension matching
        if output.shape != label.shape:
            if output.dim() == 2 and label.dim() == 3:
                output = output.unsqueeze(0)
            elif output.dim() == 3 and label.dim() == 2:
                label = label.unsqueeze(0)
            if output.shape != label.shape:
                output = output.view(label.shape)
        
        loss = loss_function(output, label)
        
        # Collect loss data
        step_history.append(step)
        loss_history.append(loss.item())
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Prepare log dictionary
        log_dict = {"Training Loss": loss.item()}
        
        # Eigenvalue computation
        hessian_time = 0
        if step in selected_steps:
            hessian_start = time.time()
            eigenvalue_step_history.append(step)
            
            eigenvalues, eigenvectors, dominant_dim, gaps, success = compute_and_analyze_eigenvalues(
                model, loss_function, single_loader, top_k, device, config, step
            )
            
            if success:
                # Collect data
                dominant_dim_history.append(dominant_dim)
                collect_eigenvalue_data(eigenvalue_history, eigenvalues)
                
                # Prepare wandb log
                wandb_data = prepare_wandb_log(eigenvalues, dominant_dim, gaps)
                log_dict.update(wandb_data)
                log_dict[f"max_hessian_{method}"] = eigenvalues[0].item()
                
                # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„ç‰¹å¾å‘é‡è¿›è¡Œåˆ†å±‚åˆ†æ
                if eigenvectors is not None and dominant_dim > 0:
                    print(f"ğŸ¨ Step {step}: å¼€å§‹åˆ†æå‰{dominant_dim}ä¸ªç‰¹å¾å‘é‡çš„åˆ†å±‚åˆ†å¸ƒ...")
                    
                    analysis_data = analyze_dominant_space_layerwise(
                        model, eigenvectors, eigenvalues[:dominant_dim], dominant_dim, IMAGE_SAVE_DIR, step
                    )
                    
                    if analysis_data is not None:
                        print(f"âœ… Step {step}: ç‰¹å¾å‘é‡åˆ†å±‚åˆ†æå®Œæˆ")
                        
                        # æ–°å¢ï¼šä¿å­˜ç‰¹å¾å‘é‡æ•°æ®
                        save_paths = save_eigenvector_data(
                            analysis_data, IMAGE_SAVE_DIR, step, method, lr, var, seed, rank, num_layer
                        )
                        
                        # è®°å½•ä¿å­˜è·¯å¾„åˆ°wandb
                        if save_paths:
                            wandb.log({
                                "EigenvectorSave/torch_path": save_paths['torch_path'],
                                "EigenvectorSave/json_path": save_paths['json_path'],
                                "EigenvectorSave/numpy_path": save_paths['numpy_path']
                            }, step=step)
                        
                        # è®°å½•åˆ†å±‚åˆ†æç»“æœåˆ°wandb
                        layer_norm_summary = {}
                        for layer_idx, norms in analysis_data['layer_norms'].items():
                            layer_name = analysis_data['layer_info'][layer_idx]['name']
                            # è®°å½•æ¯å±‚çš„ç‰¹å¾å‘é‡èŒƒæ•°æ€»å’Œ
                            layer_norm_summary[f"LayerNorm/{layer_name}"] = sum(norms)
                            # è®°å½•æ¯å±‚æ¯ä¸ªç‰¹å¾å‘é‡çš„èŒƒæ•°
                            for k, norm in enumerate(norms):
                                layer_norm_summary[f"LayerNorm/{layer_name}/EigenVec_{k+1}"] = norm
                        
                        wandb.log(layer_norm_summary, step=step)
                
                hessian_time = time.time() - hessian_start
        
        # Log to wandb
        wandb.log(log_dict, step=step)
        
        # Update progress bar
        step_time = time.time() - step_start_time
        postfix_dict = {'Loss': f'{loss.item():.4f}'}
        if step in selected_steps:
            postfix_dict['EigenTime'] = f'{hessian_time:.1f}s'
        progress_bar.set_postfix(postfix_dict)

    progress_bar.close()
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    
    # Generate visualizations
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        if len(loss_history) > 0:
            plot_training_loss(loss_history, step_history, method, lr, var, seed, rank, num_layer, config, IMAGE_SAVE_DIR, wandb)
        
        if len(eigenvalue_step_history) > 0:
            plot_top_k_eigenvalues(eigenvalue_history, eigenvalue_step_history, top_k, method, lr, var, seed, rank, num_layer, config, IMAGE_SAVE_DIR, wandb)
        
        if len(dominant_dim_history) > 0:
            plot_dominant_dims(dominant_dim_history, eigenvalue_step_history, top_k, method, lr, var, seed, rank, num_layer, config, IMAGE_SAVE_DIR, wandb)
            
        if len(eigenvalue_step_history) > 0 and eigenvalue_history:
            # ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬ä¿å­˜CSV
            from plotting import save_eigenvalue_csv_safe
            csv_path = save_eigenvalue_csv_safe(eigenvalue_history, eigenvalue_step_history, method, lr, var, seed, rank, num_layer, config, IMAGE_SAVE_DIR)
            
            if csv_path:
                print(f"âœ… CSVæ–‡ä»¶ä¿å­˜æˆåŠŸ: {csv_path}")
            else:
                print("âš ï¸  CSVæ–‡ä»¶ä¿å­˜å¤±è´¥æˆ–è·³è¿‡")

        print(f"âœ… å¯è§†åŒ–å®Œæˆ!")
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")

    # Complete training
    total_time = time.time() - total_start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {format_time(total_time)}")
    print(f"ğŸ“ å›¾ç‰‡ä¿å­˜åˆ°: {IMAGE_SAVE_DIR}")
    
    wandb.finish()

if __name__ == "__main__":
    main()