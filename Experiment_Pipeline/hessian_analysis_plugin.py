import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os
import time
from typing import Dict, List, Tuple, Optional

def create_analysis_directories(base_dir: str, step: int) -> Dict[str, str]:
    """
    åˆ›å»ºåˆ†æç»“æœçš„ç›®å½•ç»“æ„
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
        step: å½“å‰æ­¥æ•°
    
    Returns:
        åŒ…å«å„ç§ç›®å½•è·¯å¾„çš„å­—å…¸
    """
    dirs = {
        'hessian_matrices': os.path.join(base_dir, 'hessian_matrices', f'step_{step}'),
        'eigenvalues': os.path.join(base_dir, 'eigenvalues'),
        'eigenvectors': os.path.join(base_dir, 'eigenvectors', f'step_{step}'),
        'heatmaps': os.path.join(base_dir, 'heatmaps', f'step_{step}'),
        'projections': os.path.join(base_dir, 'projections', f'step_{step}'),
        'analysis': os.path.join(base_dir, 'analysis')
    }
    
    # åˆ›å»ºæ‰€æœ‰ç›®å½•
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    return dirs

def get_parameter_blocks(model) -> List[Tuple[str, torch.Tensor]]:
    """
    è·å–æ¨¡å‹çš„å‚æ•°åˆ†å—ä¿¡æ¯
    
    Args:
        model: PyTorchæ¨¡å‹
    
    Returns:
        å‚æ•°å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«(å±‚åç§°, å‚æ•°å¼ é‡)
    """
    blocks = []
    for name, param in model.named_parameters():
        if param.requires_grad:
            blocks.append((name, param))
    return blocks

def compute_parameter_ranges(blocks: List[Tuple[str, torch.Tensor]]) -> Dict[str, Tuple[int, int]]:
    """
    è®¡ç®—æ¯ä¸ªå‚æ•°å—åœ¨å±•å¹³å‚æ•°å‘é‡ä¸­çš„èŒƒå›´
    
    Args:
        blocks: å‚æ•°å—åˆ—è¡¨
    
    Returns:
        æ¯ä¸ªå‚æ•°å—çš„èµ·å§‹å’Œç»“æŸä½ç½®
    """
    ranges = {}
    current_pos = 0
    
    for name, param in blocks:
        param_size = param.numel()
        ranges[name] = (current_pos, current_pos + param_size)
        current_pos += param_size
    
    return ranges

def compute_full_hessian_with_blocks(loss, params, device=None) -> Tuple[torch.Tensor, Dict]:
    """
    è®¡ç®—å®Œæ•´HessiançŸ©é˜µåŠå…¶åˆ†å—ä¿¡æ¯
    
    Args:
        loss: æŸå¤±å‡½æ•°å€¼
        params: æ¨¡å‹å‚æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        (å®Œæ•´HessiançŸ©é˜µ, åˆ†å—ä¿¡æ¯å­—å…¸)
    """
    if device is None:
        device = loss.device
    
    params = list(params)
    params = [p.to(device) if p.device != device else p for p in params]
    
    print(f"ğŸ” è®¡ç®—å®Œæ•´HessiançŸ©é˜µåŠåˆ†å—ä¿¡æ¯...")
    
    # è·å–å‚æ•°å—ä¿¡æ¯
    param_blocks = []
    param_ranges = {}
    current_pos = 0
    
    for i, p in enumerate(params):
        param_size = p.numel()
        block_name = f"layer_{i}"
        param_blocks.append((block_name, p))
        param_ranges[block_name] = (current_pos, current_pos + param_size)
        current_pos += param_size
    
    # è®¡ç®—æ¢¯åº¦
    grads_flat = []
    for p in params:
        g = torch.autograd.grad(loss, p, create_graph=True, retain_graph=True)[0]
        grads_flat.append(g.view(-1))
    
    grads_flat = torch.cat(grads_flat).to(device)
    total_params = grads_flat.size(0)
    
    # æ„å»ºHessiançŸ©é˜µ
    hessian_matrix = torch.zeros((total_params, total_params), device=device, dtype=torch.float32)
    
    print(f"   æ€»å‚æ•°æ•°: {total_params}")
    
    for i, g in enumerate(grads_flat):
        if i % max(1, total_params // 10) == 0:
            print(f"   è¿›åº¦: {i}/{total_params} ({100*i/total_params:.1f}%)")
        
        hessian_row = torch.autograd.grad(
            outputs=g, 
            inputs=params, 
            retain_graph=True, 
            allow_unused=True
        )
        
        hessian_row_flat = []
        for h, p in zip(hessian_row, params):
            if h is None:
                h_flat = torch.zeros_like(p, device=device).view(-1)
            else:
                h_flat = h.to(device).view(-1)
            hessian_row_flat.append(h_flat)
        
        hessian_matrix[i] = torch.cat(hessian_row_flat)
    
    print(f"âœ… HessiançŸ©é˜µè®¡ç®—å®Œæˆ")
    
    return hessian_matrix, param_ranges

def extract_hessian_blocks(hessian_matrix: torch.Tensor, param_ranges: Dict[str, Tuple[int, int]]) -> Dict[str, torch.Tensor]:
    """
    ä»å®Œæ•´HessiançŸ©é˜µä¸­æå–å„å±‚çš„åˆ†å—
    
    Args:
        hessian_matrix: å®Œæ•´HessiançŸ©é˜µ
        param_ranges: å‚æ•°èŒƒå›´å­—å…¸
    
    Returns:
        å„å±‚Hessianåˆ†å—çš„å­—å…¸
    """
    blocks = {}
    layer_names = list(param_ranges.keys())
    
    print(f"ğŸ“Š æå–Hessianåˆ†å—çŸ©é˜µ...")
    
    # æå–å¯¹è§’å—ï¼ˆæ¯å±‚å†…éƒ¨çš„Hessianï¼‰
    for layer_name, (start, end) in param_ranges.items():
        blocks[f"{layer_name}_diagonal"] = hessian_matrix[start:end, start:end].clone()
        print(f"   {layer_name} å¯¹è§’å—: {blocks[f'{layer_name}_diagonal'].shape}")
    
    # æå–éå¯¹è§’å—ï¼ˆå±‚é—´äº¤äº’çš„Hessianï¼‰
    for i, layer1 in enumerate(layer_names):
        for j, layer2 in enumerate(layer_names):
            if i < j:  # åªä¿å­˜ä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œé¿å…é‡å¤
                start1, end1 = param_ranges[layer1]
                start2, end2 = param_ranges[layer2]
                block_name = f"{layer1}_to_{layer2}"
                blocks[block_name] = hessian_matrix[start1:end1, start2:end2].clone()
                print(f"   {block_name} äº¤äº’å—: {blocks[block_name].shape}")
    
    return blocks

def plot_hessian_heatmaps(hessian_blocks: Dict[str, torch.Tensor], save_dir: str, step: int, wandb_logger=None):
    """
    ç»˜åˆ¶Hessianåˆ†å—çš„çƒ­åŠ›å›¾
    """
    print(f"ğŸ¨ ç»˜åˆ¶Hessianåˆ†å—çƒ­åŠ›å›¾...")
    
    # å¼ºåˆ¶è®¾ç½®matplotlibåç«¯å’Œå­—ä½“
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
    import matplotlib.pyplot as plt
    
    # æ¸…é™¤æ‰€æœ‰å­—ä½“è®¾ç½®ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤
    plt.rcdefaults()
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    
    for block_name, block_matrix in hessian_blocks.items():
        try:
            fig, ax = plt.subplots(figsize=(8, 6))
            
            # è½¬æ¢ä¸ºCPUå¹¶å–ç»å¯¹å€¼
            matrix_abs = torch.abs(block_matrix).cpu().numpy()
            
            # å¦‚æœçŸ©é˜µå¤ªå¤§ï¼Œè¿›è¡Œä¸‹é‡‡æ ·
            max_size = 50  # å‡å°å°ºå¯¸é¿å…å†…å­˜é—®é¢˜
            if matrix_abs.shape[0] > max_size or matrix_abs.shape[1] > max_size:
                step_size = max(1, max(matrix_abs.shape) // max_size)
                matrix_abs = matrix_abs[::step_size, ::step_size]
            
            # ç›´æ¥ä½¿ç”¨imshowç»˜åˆ¶ï¼Œé¿å…seaborn
            im = ax.imshow(matrix_abs, cmap='Blues', aspect='auto')
            plt.colorbar(im, ax=ax)
            
            # ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
            ax.set_title(f'Hessian Block: {block_name} Step {step}')
            ax.set_xlabel('Parameter Index')
            ax.set_ylabel('Parameter Index')
            
            # ä¿å­˜å›¾ç‰‡
            save_path = os.path.join(save_dir, f'hessian_heatmap_{block_name}_step_{step}.png')
            plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
            
            # æ·»åŠ è¿™å‡ è¡Œï¼šä¸Šä¼ åˆ°SwanLab
            if wandb_logger:
                wandb_logger.log({
                    f"Hessian_Heatmaps/Block_{block_name}_Step_{step}": wandb_logger.Image(save_path)
                })
            
            plt.close()
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                print(f"   âœ… çƒ­åŠ›å›¾ä¿å­˜æˆåŠŸ: {save_path} ({file_size} bytes)")
            else:
                print(f"   âŒ çƒ­åŠ›å›¾ä¿å­˜å¤±è´¥: {save_path}")
                
        except Exception as e:
            print(f"   âŒ ç»˜åˆ¶ {block_name} çƒ­åŠ›å›¾å¤±è´¥: {e}")
            continue

def plot_global_hessian_heatmap(hessian_matrix: torch.Tensor, save_dir: str, step: int, wandb_logger=None):
    """
    ç»˜åˆ¶å®Œæ•´çš„å…¨å±€HessiançŸ©é˜µçƒ­åŠ›å›¾ - çº¿æ€§åˆ»åº¦ï¼Œç»å¯¹å€¼
    """
    print(f"ğŸ¨ ç»˜åˆ¶å…¨å±€Hessiançƒ­åŠ›å›¾...")
    
    # å¼ºåˆ¶è®¾ç½®matplotlibåç«¯å’Œå­—ä½“
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    
    # æ¸…é™¤æ‰€æœ‰å­—ä½“è®¾ç½®
    plt.rcdefaults()
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    
    try:
        # è½¬æ¢ä¸ºCPUå¹¶å–ç»å¯¹å€¼
        matrix_abs = torch.abs(hessian_matrix).cpu().numpy()
        
        print(f"   å…¨å±€HessiançŸ©é˜µå¤§å°: {matrix_abs.shape}")
        
        # å¦‚æœçŸ©é˜µå¤ªå¤§ï¼Œè¿›è¡Œä¸‹é‡‡æ ·
        max_size = 200  # å…¨å±€çŸ©é˜µå¯ä»¥ç¨å¤§ä¸€äº›
        if matrix_abs.shape[0] > max_size or matrix_abs.shape[1] > max_size:
            step_size = max(1, max(matrix_abs.shape) // max_size)
            matrix_abs = matrix_abs[::step_size, ::step_size]
            print(f"   ä¸‹é‡‡æ ·åå¤§å°: {matrix_abs.shape}")
        
        # 1. ç»˜åˆ¶å®Œæ•´çƒ­åŠ›å›¾ - çº¿æ€§åˆ»åº¦ï¼Œè“è‰²é…è‰²
        fig, ax = plt.subplots(figsize=(10, 8))
        
        im = ax.imshow(matrix_abs, cmap='Blues', aspect='auto')
        plt.colorbar(im, ax=ax, label='|Hessian|')
        
        ax.set_title(f'Global Hessian Matrix (Linear Scale) Step {step}')
        ax.set_xlabel('Parameter Index')
        ax.set_ylabel('Parameter Index')
        
        # ä¿å­˜å®Œæ•´çƒ­åŠ›å›¾
        save_path = os.path.join(save_dir, f'global_hessian_full_step_{step}.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        
        # ä¸Šä¼ åˆ°SwanLab
        if wandb_logger:
            wandb_logger.log({
                f"Global_Hessian/Full_Matrix_Step_{step}": wandb_logger.Image(save_path)
            })
        
        plt.close()
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(save_path):
            file_size = os.path.getsize(save_path)
            print(f"   âœ… å…¨å±€çƒ­åŠ›å›¾ä¿å­˜æˆåŠŸ: {save_path} ({file_size} bytes)")
        else:
            print(f"   âŒ å…¨å±€çƒ­åŠ›å›¾ä¿å­˜å¤±è´¥: {save_path}")
        
        # 2. ç»˜åˆ¶å¯¹æ¯”å›¾ï¼ˆå…¨å›¾ vs å¯¹è§’çº¿åŒºåŸŸï¼‰
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å·¦å›¾ï¼šå®Œæ•´çŸ©é˜µ
        im1 = ax1.imshow(matrix_abs, cmap='Blues', aspect='auto')
        plt.colorbar(im1, ax=ax1, label='|Hessian|')
        ax1.set_title(f'Full Hessian Matrix Step {step}')
        ax1.set_xlabel('Parameter Index')
        ax1.set_ylabel('Parameter Index')
        
        # å³å›¾ï¼šå¯¹è§’çº¿åŒºåŸŸæ”¾å¤§
        diag_size = min(100, matrix_abs.shape[0] // 2)
        center = matrix_abs.shape[0] // 2
        start_idx = max(0, center - diag_size // 2)
        end_idx = min(matrix_abs.shape[0], center + diag_size // 2)
        
        diag_region = matrix_abs[start_idx:end_idx, start_idx:end_idx]
        im2 = ax2.imshow(diag_region, cmap='Blues', aspect='auto')
        plt.colorbar(im2, ax=ax2, label='|Hessian|')
        ax2.set_title(f'Central Diagonal Region Step {step}')
        ax2.set_xlabel('Parameter Index')
        ax2.set_ylabel('Parameter Index')
        
        # ä¿å­˜å¯¹æ¯”å›¾
        save_path_combined = os.path.join(save_dir, f'global_hessian_combined_step_{step}.png')
        plt.savefig(save_path_combined, dpi=150, bbox_inches='tight', facecolor='white')
        
        # ä¸Šä¼ åˆ°SwanLab
        if wandb_logger:
            wandb_logger.log({
                f"Global_Hessian/Combined_View_Step_{step}": wandb_logger.Image(save_path_combined)
            })
        
        plt.close()
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(save_path_combined):
            file_size = os.path.getsize(save_path_combined)
            print(f"   âœ… ç»„åˆçƒ­åŠ›å›¾ä¿å­˜æˆåŠŸ: {save_path_combined} ({file_size} bytes)")
        else:
            print(f"   âŒ ç»„åˆçƒ­åŠ›å›¾ä¿å­˜å¤±è´¥: {save_path_combined}")
            
    except Exception as e:
        print(f"   âŒ ç»˜åˆ¶å…¨å±€Hessiançƒ­åŠ›å›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def compute_eigenvalue_projections(eigenvectors: torch.Tensor, param_ranges: Dict[str, Tuple[int, int]], dominant_dim: int) -> Dict[str, torch.Tensor]:
    """
    è®¡ç®—ç‰¹å¾å‘é‡åœ¨å„å±‚å‚æ•°ä¸Šçš„æŠ•å½±èŒƒæ•°
    
    Args:
        eigenvectors: ç‰¹å¾å‘é‡çŸ©é˜µ
        param_ranges: å‚æ•°èŒƒå›´å­—å…¸
        dominant_dim: dominantç©ºé—´ç»´åº¦
    
    Returns:
        å„å±‚æŠ•å½±èŒƒæ•°çš„å­—å…¸
    """
    print(f"ğŸ“Š è®¡ç®—ç‰¹å¾å‘é‡åœ¨å„å±‚çš„æŠ•å½±èŒƒæ•°...")
    
    projections = {}
    
    # åªè®¡ç®—dominant spaceçš„ç‰¹å¾å‘é‡
    # dominant_eigenvectors = eigenvectors[:, :dominant_dim]
    dominant_eigenvectors = eigenvectors
    
    for layer_name, (start, end) in param_ranges.items():
        # æå–è¯¥å±‚å¯¹åº”çš„ç‰¹å¾å‘é‡åˆ†é‡
        layer_components = dominant_eigenvectors[start:end, :]
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾å‘é‡åœ¨è¯¥å±‚çš„èŒƒæ•°
        layer_norms = torch.norm(layer_components, dim=0)  # shape: (dominant_dim,)
        projections[layer_name] = layer_norms
        
        print(f"   {layer_name}: {layer_norms.shape}")
    
    return projections

def save_projections_to_csv(projections: Dict[str, torch.Tensor], save_dir: str, step: int):
    """
    å°†dominant eigenvector projectionsä¿å­˜ä¸ºCSVæ ¼å¼ - åªä¿å­˜é•¿æ ¼å¼
    
    Args:
        projections: æŠ•å½±èŒƒæ•°å­—å…¸
        save_dir: ä¿å­˜ç›®å½•
        step: å½“å‰æ­¥æ•°
    """
    print(f"ğŸ“Š ä¿å­˜æŠ•å½±æ•°æ®ä¸ºCSVæ ¼å¼...")
    
    # æå–å±‚æ•°æ®å¹¶æ’åº
    layer_data = []
    for layer_name, projection_tensor in projections.items():
        try:
            if 'layer_' in layer_name:
                layer_num = int(layer_name.split('_')[1])
                layer_data.append((layer_num, layer_name, projection_tensor.cpu().numpy()))
        except:
            continue
    
    if len(layer_data) == 0:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å±‚æŠ•å½±æ•°æ®")
        return
    
    # æŒ‰å±‚æ•°æ’åº
    layer_data.sort(key=lambda x: x[0])
    
    # è·å–dominantç‰¹å¾å‘é‡æ•°é‡
    dominant_dim = layer_data[0][2].shape[0]
    
    # ä¿å­˜é•¿æ ¼å¼CSVæ–‡ä»¶
    try:
        # å‡†å¤‡æ•°æ®
        rows = []
        for layer_num, layer_name, projections_array in layer_data:
            for eigenvec_idx in range(dominant_dim):
                rows.append({
                    'step': step,
                    'layer_number': layer_num,
                    'layer_name': layer_name,
                    'eigenvector_index': eigenvec_idx + 1,
                    'projection_norm': projections_array[eigenvec_idx]
                })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(rows)
        csv_path = os.path.join(save_dir, f'projections_step_{step}.csv')
        df.to_csv(csv_path, index=False)
        print(f"   âœ… æŠ•å½±æ•°æ®CSVä¿å­˜æˆåŠŸ: {csv_path}")
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜æŠ•å½±CSVå¤±è´¥: {e}")
        
def plot_eigenvalue_projections(projections: Dict[str, torch.Tensor], save_dir: str, step: int, eigenvalues: torch.Tensor = None, wandb_logger=None):
    """
    ç»˜åˆ¶ç‰¹å¾å‘é‡æŠ•å½±èŒƒæ•°å›¾ - ä¸ºæ¯ä¸ªdominantç‰¹å¾å‘é‡å•ç‹¬ç»˜åˆ¶
    """
    print(f"ğŸ¨ ç»˜åˆ¶ç‰¹å¾å‘é‡æŠ•å½±èŒƒæ•°å›¾...")
    
    # å¼ºåˆ¶è®¾ç½®matplotlibåç«¯å’Œå­—ä½“
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    
    # æ¸…é™¤æ‰€æœ‰å­—ä½“è®¾ç½®
    plt.rcdefaults()
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    
    layer_names = list(projections.keys())
    if len(layer_names) == 0:
        print("   âš ï¸ æ²¡æœ‰æŠ•å½±æ•°æ®")
        return
    
    # æå–å±‚æ•°å¹¶æ’åº
    layer_data = []
    for layer_name in layer_names:
        try:
            if 'layer_' in layer_name:
                layer_num = int(layer_name.split('_')[1])
                layer_data.append((layer_num, layer_name, projections[layer_name]))
        except:
            continue
    
    if len(layer_data) == 0:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å±‚æ•°æ®")
        return
    
    # æŒ‰å±‚æ•°æ’åº
    layer_data.sort(key=lambda x: x[0])
    layer_numbers = [x[0] for x in layer_data]
    dominant_dim = layer_data[0][2].shape[0]  # è·å–dominantç»´åº¦æ•°
    
    print(f"   å‘ç° {dominant_dim} ä¸ªdominantç‰¹å¾å‘é‡")
    
    # 1. ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡å•ç‹¬ç»˜åˆ¶
    for eigenvec_idx in range(dominant_dim):
        try:
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # æ”¶é›†è¿™ä¸ªç‰¹å¾å‘é‡åœ¨å„å±‚çš„æŠ•å½±
            projection_values = []
            for layer_num, layer_name, norms in layer_data:
                projection_values.append(norms[eigenvec_idx].item())
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars = ax.bar(layer_numbers, projection_values, alpha=0.7, color='skyblue')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, projection_values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + max(projection_values)*0.01,
                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)
            
            ax.set_xlabel('Layer Number', fontsize=12)
            ax.set_ylabel('Projection Norm', fontsize=12)
            
            # æ ‡é¢˜åŒ…å«ç‰¹å¾å€¼ä¿¡æ¯
            if eigenvalues is not None and eigenvec_idx < len(eigenvalues):
                eigenval = eigenvalues[eigenvec_idx].item()
                ax.set_title(f'Eigenvector {eigenvec_idx+1} Projections (Î»={eigenval:.4e}) Step {step}', fontsize=14)
            else:
                ax.set_title(f'Eigenvector {eigenvec_idx+1} Projections Step {step}', fontsize=14)
            
            ax.grid(True, alpha=0.3, axis='y')
            ax.set_xticks(layer_numbers)
            
            # ä¿å­˜å•ä¸ªç‰¹å¾å‘é‡å›¾ç‰‡
            save_path = os.path.join(save_dir, f'projection_eigenvec_{eigenvec_idx+1}_step_{step}.png')
            plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
            
            # ä¸Šä¼ åˆ°SwanLab
            if wandb_logger:
                wandb_logger.log({
                    f"Eigenvalue_Projections/Eigenvec_{eigenvec_idx+1}_Step_{step}": wandb_logger.Image(save_path)
                })
            
            plt.close()
            
            # éªŒè¯æ–‡ä»¶
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                print(f"   âœ… ç‰¹å¾å‘é‡{eigenvec_idx+1}æŠ•å½±å›¾ä¿å­˜æˆåŠŸ: {save_path} ({file_size} bytes)")
            else:
                print(f"   âŒ ç‰¹å¾å‘é‡{eigenvec_idx+1}æŠ•å½±å›¾ä¿å­˜å¤±è´¥: {save_path}")
                
        except Exception as e:
            print(f"   âŒ ç»˜åˆ¶ç‰¹å¾å‘é‡{eigenvec_idx+1}æŠ•å½±å›¾å¤±è´¥: {e}")
            continue
    
    # 2. ç»˜åˆ¶æ‰€æœ‰ç‰¹å¾å‘é‡çš„ç»„åˆå›¾
    try:
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # è®¾ç½®é¢œè‰²
        colors = plt.cm.tab10(np.linspace(0, 1, dominant_dim))
        bar_width = 0.8 / dominant_dim
        
        # ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡ç»˜åˆ¶æŸ±çŠ¶å›¾
        for eigenvec_idx in range(dominant_dim):
            projection_values = []
            for layer_num, layer_name, norms in layer_data:
                projection_values.append(norms[eigenvec_idx].item())
            
            # è®¡ç®—æ¯ä¸ªç‰¹å¾å‘é‡çš„æŸ±å­ä½ç½®åç§»
            offset = (eigenvec_idx - dominant_dim/2 + 0.5) * bar_width
            x_positions = [x + offset for x in layer_numbers]
            
            # å‡†å¤‡æ ‡ç­¾
            if eigenvalues is not None and eigenvec_idx < len(eigenvalues):
                eigenval = eigenvalues[eigenvec_idx].item()
                label = f'Eigenvec {eigenvec_idx+1} (Î»={eigenval:.2e})'
            else:
                label = f'Eigenvector {eigenvec_idx+1}'
            
            ax.bar(x_positions, projection_values, 
                  width=bar_width, label=label, 
                  alpha=0.8, color=colors[eigenvec_idx])
        
        ax.set_xlabel('Layer Number', fontsize=12)
        ax.set_ylabel('Projection Norm', fontsize=12)
        ax.set_title(f'All Dominant Eigenvectors Projections Comparison (Step {step})', fontsize=14)
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_xticks(layer_numbers)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # ä¿å­˜ç»„åˆå›¾
        save_path_combined = os.path.join(save_dir, f'projection_all_eigenvecs_step_{step}.png')
        plt.savefig(save_path_combined, dpi=150, bbox_inches='tight', facecolor='white')
        
        # ä¸Šä¼ åˆ°SwanLab
        if wandb_logger:
            wandb_logger.log({
                f"Eigenvalue_Projections/All_Eigenvecs_Step_{step}": wandb_logger.Image(save_path_combined)
            })
        
        plt.close()
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(save_path_combined):
            file_size = os.path.getsize(save_path_combined)
            print(f"   âœ… æ‰€æœ‰ç‰¹å¾å‘é‡ç»„åˆæŠ•å½±å›¾ä¿å­˜æˆåŠŸ: {save_path_combined} ({file_size} bytes)")
        else:
            print(f"   âŒ æ‰€æœ‰ç‰¹å¾å‘é‡ç»„åˆæŠ•å½±å›¾ä¿å­˜å¤±è´¥: {save_path_combined}")
            
    except Exception as e:
        print(f"   âŒ ç»˜åˆ¶æ‰€æœ‰ç‰¹å¾å‘é‡ç»„åˆæŠ•å½±å›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def compute_trace_ratio(eigenvalues: torch.Tensor, hessian_matrix: torch.Tensor, config: dict) -> Dict[str, float]:
    """
    è®¡ç®—dominantç‰¹å¾å€¼ä¹‹å’Œä¸traceçš„æ¯”å€¼ï¼Œå¹¶ä¸ç†è®ºå€¼æ¯”è¾ƒ
    
    Args:
        eigenvalues: ç‰¹å¾å€¼
        hessian_matrix: HessiançŸ©é˜µ
        config: é…ç½®å­—å…¸
    
    Returns:
        åŒ…å«æ¯”å€¼å’Œç†è®ºå€¼çš„å­—å…¸
    """
    print(f"ğŸ“Š è®¡ç®—traceæ¯”å€¼...")
    
    # è®¡ç®—trace
    trace = torch.trace(hessian_matrix).item()
    
    # è·å–é…ç½®å‚æ•°
    rank = config.get('rank', 5)
    num_layers = config.get('num_layer', 3)
    input_dim = config.get('input_dim', 10)
    hidden_dim = config.get('hidden_dim', 10)
    
    # è®¡ç®—dominantç‰¹å¾å€¼ä¹‹å’Œ
    dominant_eigenvalues_sum = torch.sum(eigenvalues).item()
    
    # è®¡ç®—æ¯”å€¼
    ratio = dominant_eigenvalues_sum / trace if trace != 0 else 0
    
    # è®¡ç®—ç†è®ºå€¼: r^L / (dâ‚€ + d_L - 1 - rÂ²)
    # å‡è®¾d_L = hidden_dim (æœ€åä¸€å±‚çš„ç»´åº¦)
    theoretical_ratio = 0 # (rank ** num_layers) / (input_dim + hidden_dim - 1 - rank**2)
    
    results = {
        'trace': trace,
        'dominant_sum': dominant_eigenvalues_sum,
        'ratio': ratio,
        'theoretical_ratio': theoretical_ratio,
        'difference': abs(ratio - theoretical_ratio)
    }
    
    print(f"   Trace: {trace:.6e}")
    print(f"   Dominantç‰¹å¾å€¼ä¹‹å’Œ: {dominant_eigenvalues_sum:.6e}")
    print(f"   å®é™…æ¯”å€¼: {ratio:.6f}")
    print(f"   ç†è®ºæ¯”å€¼: {theoretical_ratio:.6f}")
    print(f"   å·®å¼‚: {results['difference']:.6f}")
    
    return results

def compute_energy_ratio(eigenvalues: torch.Tensor, fnorm_squared: float) -> Dict[str, float]:
    """
    è®¡ç®—dominant spaceç‰¹å¾å€¼å¹³æ–¹å’Œä¸Hessian F-normå¹³æ–¹çš„æ¯”å€¼
    
    Args:
        eigenvalues: ç‰¹å¾å€¼
        fnorm_squared: HessiançŸ©é˜µçš„F-normå¹³æ–¹
    
    Returns:
        åŒ…å«èƒ½é‡æ¯”å€¼çš„å­—å…¸
    """
    print(f"ğŸ“Š è®¡ç®—èƒ½é‡æ¯”å€¼...")
    
    # è®¡ç®—dominantç‰¹å¾å€¼å¹³æ–¹å’Œ
    eigenvalues_squared_sum = torch.sum(eigenvalues ** 2).item()
    
    # è®¡ç®—æ¯”å€¼
    energy_ratio = eigenvalues_squared_sum / fnorm_squared if fnorm_squared != 0 else 0
    
    results = {
        'eigenvalues_squared_sum': eigenvalues_squared_sum,
        'fnorm_squared': fnorm_squared,
        'energy_ratio': energy_ratio
    }
    
    print(f"   Dominantç‰¹å¾å€¼å¹³æ–¹å’Œ: {eigenvalues_squared_sum:.6e}")
    print(f"   Hessian F-normÂ²: {fnorm_squared:.6e}")
    print(f"   èƒ½é‡æ¯”å€¼: {energy_ratio:.6f}")
    
    return results

def save_analysis_data(save_dirs: Dict[str, str], step: int, 
                      # hessian_matrix: torch.Tensor, 
                      # hessian_blocks: Dict[str, torch.Tensor],
                      # eigenvalues: torch.Tensor, 
                      # eigenvectors: torch.Tensor,
                      projections: Dict[str, torch.Tensor],
                      # trace_analysis: Dict[str, float],
                      # energy_analysis: Dict[str, float]
                      ):
    """
    ä¿å­˜æ‰€æœ‰åˆ†ææ•°æ®
    """
    print(f"ğŸ’¾ ä¿å­˜åˆ†ææ•°æ®...")
    
    # 1. ä¿å­˜å®Œæ•´HessiançŸ©é˜µ
    # hessian_path = os.path.join(save_dirs['hessian_matrices'], f'hessian_full_step_{step}.pt')
    # torch.save(hessian_matrix.cpu(), hessian_path)
    # print(f"   ä¿å­˜å®Œæ•´Hessian: {hessian_path}")
    
    # 2. ä¿å­˜Hessianåˆ†å—
    #for block_name, block_matrix in hessian_blocks.items():
    #    block_path = os.path.join(save_dirs['hessian_matrices'], f'hessian_block_{block_name}_step_{step}.pt')
    #    torch.save(block_matrix.cpu(), block_path)
    #    print(f"   ä¿å­˜Hessianåˆ†å—: {block_path}")
    
    # 3. ä¿å­˜ç‰¹å¾å€¼
    #eigenvals_path = os.path.join(save_dirs['eigenvalues'], f'eigenvalues_step_{step}.csv')
    #pd.DataFrame({'eigenvalue': eigenvalues.cpu().numpy()}).to_csv(eigenvals_path, index=False)
    #print(f"   ä¿å­˜ç‰¹å¾å€¼: {eigenvals_path}")
    
    # 4. ä¿å­˜ç‰¹å¾å‘é‡
    #eigenvecs_path = os.path.join(save_dirs['eigenvectors'], f'eigenvectors_step_{step}.pt')
    #torch.save(eigenvectors.cpu(), eigenvecs_path)
    #print(f"   ä¿å­˜ç‰¹å¾å‘é‡: {eigenvecs_path}")
    
    # 5. ä¿å­˜æŠ•å½±æ•°æ® (PyTorchæ ¼å¼)
    # projections_path = os.path.join(save_dirs['projections'], f'projections_step_{step}.pt')
    #torch.save({k: v.cpu() for k, v in projections.items()}, projections_path)
    #print(f"   ä¿å­˜æŠ•å½±æ•°æ®: {projections_path}")
    
    # 6. æ–°å¢ï¼šä¿å­˜æŠ•å½±æ•°æ®ä¸ºCSVæ ¼å¼ï¼ˆé•¿æ ¼å¼ï¼‰
    save_projections_to_csv(projections, save_dirs['projections'], step)
    
    # 7. ä¿å­˜åˆ†æç»“æœ
    # analysis_data = {
    #     'step': step,
    #    'trace_analysis': trace_analysis,
    #    'energy_analysis': energy_analysis
    #}
    #analysis_path = os.path.join(save_dirs['analysis'], f'analysis_step_{step}.pt')
    #torch.save(analysis_data, analysis_path)
    #print(f"   ä¿å­˜åˆ†æç»“æœ: {analysis_path}")

class LayerWiseLRScheduler:
    """åˆ†å±‚å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, optimizer, layer_lr_config: Dict[str, float]):
        self.optimizer = optimizer
        self.layer_lr_config = layer_lr_config
        
        # è®¾ç½®æ¯ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡
        for param_group, (layer_name, lr) in zip(optimizer.param_groups, layer_lr_config.items()):
            param_group['lr'] = lr
            print(f"   è®¾ç½®{layer_name}å­¦ä¹ ç‡: {lr}")
    
    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        self.optimizer.step()
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        self.optimizer.zero_grad()

def create_layerwise_optimizer(model, config: dict):
    """åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
    print(f"ğŸ”§ åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨...")
    
    # è·å–åˆ†å±‚å­¦ä¹ ç‡é…ç½®
    layer_lr_config = config.get('layer_learning_rates', {})
    
    if not layer_lr_config:
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡
        base_lr = config.get('learning_rate', 0.01)
        layer_lr_config = {f'layer_{i}': base_lr for i in range(len(list(model.parameters())))}
        print(f"   ä½¿ç”¨ç»Ÿä¸€å­¦ä¹ ç‡: {base_lr}")
    
    # ä¸ºæ¯å±‚å‚æ•°åˆ›å»ºå‚æ•°ç»„
    param_groups = []
    for i, (name, param) in enumerate(model.named_parameters()):
        if param.requires_grad:
            layer_name = f'layer_{i}'
            lr = layer_lr_config.get(layer_name, config.get('learning_rate', 0.01))
            param_groups.append({'params': [param], 'lr': lr})
            print(f"   {name}: å­¦ä¹ ç‡ = {lr}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.SGD(param_groups, momentum=0.9)
    
    return LayerWiseLRScheduler(optimizer, layer_lr_config)

def run_complete_hessian_analysis(model, loss, config: dict, step: int, 
                                eigenvalues: torch.Tensor, eigenvectors: torch.Tensor, 
                                dominant_dim: int, fnorm_squared: float, 
                                base_save_dir: str, device=None, wandb_logger=None) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„Hessianåˆ†ææµç¨‹
    
    Args:
        model: PyTorchæ¨¡å‹
        loss: æŸå¤±å€¼
        config: é…ç½®å­—å…¸
        step: å½“å‰æ­¥æ•°
        eigenvalues: å·²è®¡ç®—çš„ç‰¹å¾å€¼
        eigenvectors: å·²è®¡ç®—çš„ç‰¹å¾å‘é‡
        dominant_dim: dominantç©ºé—´ç»´åº¦
        fnorm_squared: Hessian F-normå¹³æ–¹
        base_save_dir: åŸºç¡€ä¿å­˜ç›®å½•
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    print(f"\nğŸ” å¼€å§‹å®Œæ•´Hessianåˆ†æ (Step {step})...")
    
    # 1. åˆ›å»ºç›®å½•ç»“æ„
    save_dirs = create_analysis_directories(base_save_dir, step)
    
    # 2. è®¡ç®—å®Œæ•´HessiançŸ©é˜µå’Œåˆ†å—
    hessian_matrix, param_ranges = compute_full_hessian_with_blocks(loss, model.parameters(), device)
    
    # 3. ç»˜åˆ¶å…¨å±€Hessiançƒ­åŠ›å›¾ï¼ˆæ–°å¢ï¼‰
    # plot_global_hessian_heatmap(hessian_matrix, save_dirs['heatmaps'], step, wandb_logger)
    
    # 4. æå–Hessianåˆ†å—
    # hessian_blocks = extract_hessian_blocks(hessian_matrix, param_ranges)
    
    # 5. ç»˜åˆ¶Hessianåˆ†å—çƒ­åŠ›å›¾
    # plot_hessian_heatmaps(hessian_blocks, save_dirs['heatmaps'], step, wandb_logger)
    
    # 6. è®¡ç®—ç‰¹å¾å‘é‡æŠ•å½±
    projections = compute_eigenvalue_projections(eigenvectors, param_ranges, dominant_dim)
    
    # 7. ç»˜åˆ¶æŠ•å½±å›¾
    # plot_eigenvalue_projections(projections, save_dirs['projections'], step, 
    #                            eigenvalues[:dominant_dim], wandb_logger)
    
    # 8. è®¡ç®—traceæ¯”å€¼
    trace_analysis = compute_trace_ratio(eigenvalues[:dominant_dim], hessian_matrix, config)
    
    # 9. è®¡ç®—èƒ½é‡æ¯”å€¼
    energy_analysis = compute_energy_ratio(eigenvalues[:dominant_dim], fnorm_squared)
    
    # 10. ä¿å­˜æ‰€æœ‰æ•°æ®
    save_analysis_data(save_dirs, step, projections)
    
    # 10. å‡†å¤‡è¿”å›ç»“æœ
    results = {
        # 'hessian_matrix': hessian_matrix,
        # 'hessian_blocks': hessian_blocks,
        'param_ranges': param_ranges,
        'projections': projections,
        'trace_analysis': trace_analysis,
        'energy_analysis': energy_analysis,
        'save_dirs': save_dirs
    }
    
    print(f"âœ… å®Œæ•´Hessianåˆ†æå®Œæˆ!")
    
    return results
